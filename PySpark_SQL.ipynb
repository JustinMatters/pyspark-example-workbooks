{"cells":[{"cell_type":"code","source":["import pyspark.sql.functions as fn\nfrom pyspark.sql import SparkSession\nimport pandas as pd"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":1},{"cell_type":"markdown","source":["### First lets create some dataframes to work with\n(We'll use pandas as an intermediary to avoid having to construct schema)"],"metadata":{}},{"cell_type":"code","source":["# Create a spark session\nspark_session = SparkSession.builder.getOrCreate()\n\ndf_data = {'col_1': [1,2,3,4,5], \n           'col_2': [5,4,3,2,1],\n           'col_3': ['a', 'b', 'c', 'd', 'e'],\n           'col_4': ['one', 'two', 'one', 'two', 'one']\n          }\ndf_pandas = pd.DataFrame.from_dict(df_data)\n# create spark dataframe\ndf = spark_session.createDataFrame(df_pandas)\n# let us also prepare the dataframe for use as an SQL table\ndf.createOrReplaceTempView(\"table\")\n\nleft_data = {'left_1': [1,2,3,4,5], \n           'left_2': ['one', 'two', 'one', 'two', 'one']\n          }\nleft_pandas = pd.DataFrame.from_dict(left_data)\n# create spark dataframe\nleft_df = spark_session.createDataFrame(left_pandas)\n# let us also prepare the dataframe for use as an SQL table\nleft_df.createOrReplaceTempView(\"left_table\")\n\nright_data = {'right_1': [5,4,3,2,1],\n           'right_3': ['a', 'b', 'c', 'd', 'e']\n          }\nright_pandas = pd.DataFrame.from_dict(right_data)\n# create spark dataframe\nright_df = spark_session.createDataFrame(right_pandas)\n# let us also prepare the dataframe for use as an SQL table\nright_df.createOrReplaceTempView(\"right_table\")\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":3},{"cell_type":"markdown","source":["## Now we have our dataframes and sql tables let us use them to compare and contrast SQL and PySpark syntax"],"metadata":{}},{"cell_type":"markdown","source":["### Comparison of table description syntax"],"metadata":{}},{"cell_type":"code","source":["# pyspark\n#df_pyspark = \ndf.printSchema()\n#df_pyspark.show()\n\n# SQL\ndf_sql = spark.sql(\"DESCRIBE table\")\ndf_sql.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">root\n-- col_1: long (nullable = true)\n-- col_2: long (nullable = true)\n-- col_3: string (nullable = true)\n-- col_4: string (nullable = true)\n\n+--------+---------+-------+\ncol_name|data_type|comment|\n+--------+---------+-------+\n   col_1|   bigint|   null|\n   col_2|   bigint|   null|\n   col_3|   string|   null|\n   col_4|   string|   null|\n+--------+---------+-------+\n\n</div>"]}}],"execution_count":6},{"cell_type":"markdown","source":["### Comparison of select syntax including aliasing"],"metadata":{}},{"cell_type":"code","source":["# pyspark\ndf_pyspark = df.select(fn.col(\"col_1\").alias(\"f1\"), fn.col(\"col_3\").alias(\"f3\"))\ndf_pyspark.show()\n\n# SQL\ndf_sql = spark.sql(\"SELECT col_1 AS f1, col_3 AS f3 FROM table\")\ndf_sql.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---+---+\n f1| f3|\n+---+---+\n  1|  a|\n  2|  b|\n  3|  c|\n  4|  d|\n  5|  e|\n+---+---+\n\n+---+---+\n f1| f3|\n+---+---+\n  1|  a|\n  2|  b|\n  3|  c|\n  4|  d|\n  5|  e|\n+---+---+\n\n</div>"]}}],"execution_count":8},{"cell_type":"markdown","source":["### Selection of distinct results"],"metadata":{}},{"cell_type":"code","source":["# pyspark\ndf_pyspark = df.select(fn.col(\"col_4\")).distinct()\ndf_pyspark.show()\n\n# SQL\ndf_sql = spark.sql(\"SELECT DISTINCT col_4 FROM table\")\ndf_sql.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----+\ncol_4|\n+-----+\n  two|\n  one|\n+-----+\n\n+-----+\ncol_4|\n+-----+\n  two|\n  one|\n+-----+\n\n</div>"]}}],"execution_count":10},{"cell_type":"markdown","source":["### Comparison of limiting the number of returns"],"metadata":{}},{"cell_type":"code","source":["# pyspark\ndf_pyspark = df.limit(2)\ndf_pyspark.show()\n\n# SQL\ndf_sql = spark.sql(\"SELECT * FROM table LIMIT 2\")\ndf_sql.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----+-----+-----+-----+\ncol_1|col_2|col_3|col_4|\n+-----+-----+-----+-----+\n    1|    5|    a|  one|\n    2|    4|    b|  two|\n+-----+-----+-----+-----+\n\n+-----+-----+-----+-----+\ncol_1|col_2|col_3|col_4|\n+-----+-----+-----+-----+\n    1|    5|    a|  one|\n    2|    4|    b|  two|\n+-----+-----+-----+-----+\n\n</div>"]}}],"execution_count":12},{"cell_type":"markdown","source":["### Comparison of ordering syntax\nDefault ordering is ascending in each case"],"metadata":{}},{"cell_type":"code","source":["# pyspark\ndf_pyspark = df.orderBy(\"col_2\")\ndf_pyspark.show()\n\n# SQL\ndf_sql = spark.sql(\"SELECT * FROM table ORDER BY col_2\")\ndf_sql.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----+-----+-----+-----+\ncol_1|col_2|col_3|col_4|\n+-----+-----+-----+-----+\n    5|    1|    e|  one|\n    4|    2|    d|  two|\n    3|    3|    c|  one|\n    2|    4|    b|  two|\n    1|    5|    a|  one|\n+-----+-----+-----+-----+\n\n+-----+-----+-----+-----+\ncol_1|col_2|col_3|col_4|\n+-----+-----+-----+-----+\n    5|    1|    e|  one|\n    4|    2|    d|  two|\n    3|    3|    c|  one|\n    2|    4|    b|  two|\n    1|    5|    a|  one|\n+-----+-----+-----+-----+\n\n</div>"]}}],"execution_count":14},{"cell_type":"code","source":["# pyspark\ndf_pyspark = df.orderBy(\"col_1\", ascending = False)\ndf_pyspark.show()\n\n# SQL\ndf_sql = spark.sql(\"SELECT * FROM table ORDER BY col_1 DESC\")\ndf_sql.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----+-----+-----+-----+\ncol_1|col_2|col_3|col_4|\n+-----+-----+-----+-----+\n    5|    1|    e|  one|\n    4|    2|    d|  two|\n    3|    3|    c|  one|\n    2|    4|    b|  two|\n    1|    5|    a|  one|\n+-----+-----+-----+-----+\n\n+-----+-----+-----+-----+\ncol_1|col_2|col_3|col_4|\n+-----+-----+-----+-----+\n    5|    1|    e|  one|\n    4|    2|    d|  two|\n    3|    3|    c|  one|\n    2|    4|    b|  two|\n    1|    5|    a|  one|\n+-----+-----+-----+-----+\n\n</div>"]}}],"execution_count":15},{"cell_type":"markdown","source":["### Comparison of filter syntax"],"metadata":{}},{"cell_type":"code","source":["# PySpark\n# note that all the following achieve the same result (pick the syntax you prefer)\ndf_pyspark = df.filter(df.col_1 > 3)\ndf_pyspark.show()\ndf_pyspark = df.filter(fn.col(\"col_1\") > fn.lit(3))\ndf_pyspark.show()\ndf_pyspark = df.filter(df[\"col_1\"] > fn.lit(3))\ndf_pyspark.show()\n# we can even use operators\nfrom operator import *\ndf_pyspark = df.filter(gt(df.col_1,3))\ndf_pyspark.show()\n\n# SQL\ndf_sql = spark.sql(\"SELECT * FROM table WHERE col_1 > 3\")\ndf_sql.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----+-----+-----+-----+\ncol_1|col_2|col_3|col_4|\n+-----+-----+-----+-----+\n    4|    2|    d|  two|\n    5|    1|    e|  one|\n+-----+-----+-----+-----+\n\n+-----+-----+-----+-----+\ncol_1|col_2|col_3|col_4|\n+-----+-----+-----+-----+\n    4|    2|    d|  two|\n    5|    1|    e|  one|\n+-----+-----+-----+-----+\n\n+-----+-----+-----+-----+\ncol_1|col_2|col_3|col_4|\n+-----+-----+-----+-----+\n    4|    2|    d|  two|\n    5|    1|    e|  one|\n+-----+-----+-----+-----+\n\n+-----+-----+-----+-----+\ncol_1|col_2|col_3|col_4|\n+-----+-----+-----+-----+\n    4|    2|    d|  two|\n    5|    1|    e|  one|\n+-----+-----+-----+-----+\n\n+-----+-----+-----+-----+\ncol_1|col_2|col_3|col_4|\n+-----+-----+-----+-----+\n    4|    2|    d|  two|\n    5|    1|    e|  one|\n+-----+-----+-----+-----+\n\n</div>"]}}],"execution_count":17},{"cell_type":"markdown","source":["### Comparison of group by syntax"],"metadata":{}},{"cell_type":"code","source":["# PySpark\n# note that all the following achieve the same result (pick the syntax you prefer)\ndf_pyspark = df.groupBy(\"col_4\").agg(fn.count(\"col_1\"), fn.sum(\"col_2\"))\ndf_pyspark.show()\n\n# SQL\ndf_sql = spark.sql(\"SELECT col_4, COUNT(col_1), SUM (col_2) FROM table GROUP BY col_4\")\ndf_sql.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----+------------+----------+\ncol_4|count(col_1)|sum(col_2)|\n+-----+------------+----------+\n  two|           2|         6|\n  one|           3|         9|\n+-----+------------+----------+\n\n+-----+------------+----------+\ncol_4|count(col_1)|sum(col_2)|\n+-----+------------+----------+\n  two|           2|         6|\n  one|           3|         9|\n+-----+------------+----------+\n\n</div>"]}}],"execution_count":19},{"cell_type":"markdown","source":["### Comparison of table join syntax\nNote that row order is not preserved in either case. Useful reference at http://www.learnbymarketing.com/1100/pyspark-joins-by-example/"],"metadata":{}},{"cell_type":"markdown","source":["### Inner join"],"metadata":{}},{"cell_type":"code","source":["# PySpark\n# note that Pyspark  join defaults to inner join\ndf_pyspark = left_df.join(right_df, left_df.left_1 == right_df.right_1)\ndf_pyspark.show()\n\n# SQL\ndf_sql = spark.sql(\"SELECT * FROM left_table INNER JOIN right_table ON left_table.left_1 = right_table.right_1\")\ndf_sql.show()\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+------+------+-------+-------+\nleft_1|left_2|right_1|right_3|\n+------+------+-------+-------+\n     5|   one|      5|      a|\n     1|   one|      1|      e|\n     3|   one|      3|      c|\n     2|   two|      2|      d|\n     4|   two|      4|      b|\n+------+------+-------+-------+\n\n+------+------+-------+-------+\nleft_1|left_2|right_1|right_3|\n+------+------+-------+-------+\n     5|   one|      5|      a|\n     1|   one|      1|      e|\n     3|   one|      3|      c|\n     2|   two|      2|      d|\n     4|   two|      4|      b|\n+------+------+-------+-------+\n\n</div>"]}}],"execution_count":22},{"cell_type":"markdown","source":["### Outer join"],"metadata":{}},{"cell_type":"code","source":["# PySpark\n# use the how keyword to specify join type\ndf_pyspark = left_df.join(right_df, left_df.left_1 == right_df.right_1, how = \"outer\")\ndf_pyspark.show()\n\n# SQL - note the use of FULL\ndf_sql = spark.sql(\"SELECT * FROM left_table FULL OUTER JOIN right_table ON left_1 = right_1\")\ndf_sql.show()\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+------+------+-------+-------+\nleft_1|left_2|right_1|right_3|\n+------+------+-------+-------+\n     5|   one|      5|      a|\n     1|   one|      1|      e|\n     3|   one|      3|      c|\n     2|   two|      2|      d|\n     4|   two|      4|      b|\n+------+------+-------+-------+\n\n+------+------+-------+-------+\nleft_1|left_2|right_1|right_3|\n+------+------+-------+-------+\n     5|   one|      5|      a|\n     1|   one|      1|      e|\n     3|   one|      3|      c|\n     2|   two|      2|      d|\n     4|   two|      4|      b|\n+------+------+-------+-------+\n\n</div>"]}}],"execution_count":24},{"cell_type":"markdown","source":["### Left join"],"metadata":{}},{"cell_type":"code","source":["# PySpark\n# note that Pyspark  join defaults to inner join\n# use & and | to represent and AND and OR conditions\ndf_pyspark = left_df.join(right_df, left_df.left_1 == right_df.right_1, how = \"left\")\ndf_pyspark.show()\n\n# SQL\ndf_sql = spark.sql(\"SELECT * FROM left_table LEFT JOIN right_table ON left_1 = right_1\")\ndf_sql.show()\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+------+------+-------+-------+\nleft_1|left_2|right_1|right_3|\n+------+------+-------+-------+\n     5|   one|      5|      a|\n     1|   one|      1|      e|\n     3|   one|      3|      c|\n     2|   two|      2|      d|\n     4|   two|      4|      b|\n+------+------+-------+-------+\n\n+------+------+-------+-------+\nleft_1|left_2|right_1|right_3|\n+------+------+-------+-------+\n     5|   one|      5|      a|\n     1|   one|      1|      e|\n     3|   one|      3|      c|\n     2|   two|      2|      d|\n     4|   two|      4|      b|\n+------+------+-------+-------+\n\n</div>"]}}],"execution_count":26},{"cell_type":"markdown","source":["### Cross join"],"metadata":{}},{"cell_type":"code","source":["# PySpark\n# note that Pyspark  has a specific command for the cross join\ndf_pyspark = left_df.crossJoin(right_df)\ndf_pyspark.show()\n\n# SQL\ndf_sql = spark.sql(\"SELECT * FROM left_table CROSS JOIN right_table\")\ndf_sql.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+------+------+-------+-------+\nleft_1|left_2|right_1|right_3|\n+------+------+-------+-------+\n     1|   one|      5|      a|\n     1|   one|      4|      b|\n     1|   one|      3|      c|\n     1|   one|      2|      d|\n     1|   one|      1|      e|\n     2|   two|      5|      a|\n     2|   two|      4|      b|\n     2|   two|      3|      c|\n     2|   two|      2|      d|\n     2|   two|      1|      e|\n     3|   one|      5|      a|\n     3|   one|      4|      b|\n     3|   one|      3|      c|\n     3|   one|      2|      d|\n     3|   one|      1|      e|\n     4|   two|      5|      a|\n     4|   two|      4|      b|\n     4|   two|      3|      c|\n     4|   two|      2|      d|\n     4|   two|      1|      e|\n+------+------+-------+-------+\nonly showing top 20 rows\n\n+------+------+-------+-------+\nleft_1|left_2|right_1|right_3|\n+------+------+-------+-------+\n     1|   one|      5|      a|\n     1|   one|      4|      b|\n     1|   one|      3|      c|\n     1|   one|      2|      d|\n     1|   one|      1|      e|\n     2|   two|      5|      a|\n     2|   two|      4|      b|\n     2|   two|      3|      c|\n     2|   two|      2|      d|\n     2|   two|      1|      e|\n     3|   one|      5|      a|\n     3|   one|      4|      b|\n     3|   one|      3|      c|\n     3|   one|      2|      d|\n     3|   one|      1|      e|\n     4|   two|      5|      a|\n     4|   two|      4|      b|\n     4|   two|      3|      c|\n     4|   two|      2|      d|\n     4|   two|      1|      e|\n+------+------+-------+-------+\nonly showing top 20 rows\n\n</div>"]}}],"execution_count":28},{"cell_type":"markdown","source":["### Concatenation of dataframes"],"metadata":{}},{"cell_type":"code","source":["# UNION\n\n# PySpark\n# note that Pyspark Union is a union all so use distinct to get union\ndf_pyspark = left_df.select(\"left_1\").union(right_df.select(\"right_1\")).distinct()\ndf_pyspark.show()\n\n# SQL\ndf_sql = spark.sql(\"SELECT left_1 FROM left_table UNION SELECT Right_1 FROM right_table\")\ndf_sql.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+------+\nleft_1|\n+------+\n     5|\n     1|\n     3|\n     2|\n     4|\n+------+\n\n+------+\nleft_1|\n+------+\n     5|\n     1|\n     3|\n     2|\n     4|\n+------+\n\n</div>"]}}],"execution_count":30},{"cell_type":"code","source":["# UNION ALL\n\n# PySpark\n# note that PySpark union performs union all (PySpark unionAll is deprecated)\ndf_pyspark = left_df.select(\"left_1\").union(right_df.select(\"right_1\"))\ndf_pyspark.show()\n\n# SQL\ndf_sql = spark.sql(\"SELECT left_1 FROM left_table UNION ALL SELECT Right_1 FROM right_table\")\ndf_sql.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+------+\nleft_1|\n+------+\n     1|\n     2|\n     3|\n     4|\n     5|\n     5|\n     4|\n     3|\n     2|\n     1|\n+------+\n\n+------+\nleft_1|\n+------+\n     1|\n     2|\n     3|\n     4|\n     5|\n     5|\n     4|\n     3|\n     2|\n     1|\n+------+\n\n</div>"]}}],"execution_count":31},{"cell_type":"markdown","source":["### Amending data in a table\nNote that SQL ALTER and SET commands will not work on Pyspark dataframes because they are immutable, therefore you have to use PySpark's withColumn function and assign to a dataframe"],"metadata":{}},{"cell_type":"code","source":["# PySpark\ndf_pyspark = df.withColumn(\"col_4\", fn.when(df.col_4 == 'two', 'changed').otherwise(df.col_4))\ndf_pyspark.show()\n\n# SQL\n# IMPORTANT - ALTER and SET do not work as dataframes are immutable - here is a fallback\ndf_sql = spark.sql(\"SELECT col_1, col_2, col_3, CASE WHEN col_4 = 'two' THEN 'changed' ELSE col_4 END AS col_4 FROM table\")\ndf_sql.show()\n\n# alternatively you can combine spark.sql and pyspark commands\ndf_sql = spark.sql(\"SELECT * FROM table\").withColumn(\"col_4\", fn.when(df.col_4 == 'two', 'changed').otherwise(df.col_4))\ndf_sql.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----+-----+-----+-------+\ncol_1|col_2|col_3|  col_4|\n+-----+-----+-----+-------+\n    1|    5|    a|    one|\n    2|    4|    b|changed|\n    3|    3|    c|    one|\n    4|    2|    d|changed|\n    5|    1|    e|    one|\n+-----+-----+-----+-------+\n\n+-----+-----+-----+-------+\ncol_1|col_2|col_3|  col_4|\n+-----+-----+-----+-------+\n    1|    5|    a|    one|\n    2|    4|    b|changed|\n    3|    3|    c|    one|\n    4|    2|    d|changed|\n    5|    1|    e|    one|\n+-----+-----+-----+-------+\n\n+-----+-----+-----+-------+\ncol_1|col_2|col_3|  col_4|\n+-----+-----+-----+-------+\n    1|    5|    a|    one|\n    2|    4|    b|changed|\n    3|    3|    c|    one|\n    4|    2|    d|changed|\n    5|    1|    e|    one|\n+-----+-----+-----+-------+\n\n</div>"]}}],"execution_count":33},{"cell_type":"markdown","source":["## Further reading\n###https://docs.databricks.com/spark/latest/dataframes-datasets/introduction-to-dataframes-python.html\n###https://spark.apache.org/docs/2.2.0/sql-programming-guide.html"],"metadata":{}},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":35}],"metadata":{"name":"PySpark_SQL","notebookId":3433231071204354},"nbformat":4,"nbformat_minor":0}
