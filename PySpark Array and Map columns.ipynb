{"cells":[{"cell_type":"code","source":["import pyspark.sql.functions as fn"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":1},{"cell_type":"code","source":["# set a spark context\nsc = spark.sparkContext\n\n# define our data as JSON\npeople_json = ['[{\"name\":{\"first\":\"Alice\", \"last\": \"Summers\"},\"age\": \"34\", \"cars\": [\"honda\"],  \"locations\":[{\"type\": \"work\", \"city\":\"Glasgow\"}, {\"type\": \"home\", \"city\": \"Edinburgh\"}]},\\\n                 {\"name\":{\"first\":\"Bob\", \"last\": \"Winters\"},\"age\": \"33\", \"cars\": [\"ford\", \"BMW\"], \"locations\":[{\"type\": \"work\", \"city\":\"Glasgow\"}, {\"type\": \"home\", \"city\": \"Edinburgh\"}]},\\\n                 {\"name\":{\"first\":\"Charlie\", \"last\": \"Spring\"},\"age\": \"35\", \"cars\": [\"vauxhall\", \"peugeot\", \"VW\"], \"locations\":[{\"type\": \"work\", \"city\":\"Bristol\"}, {\"type\": \"home\", \"city\": \"Bath\"}, {\"type\":\"holiday\", \"city\": \"Paris\"}]}]']\n\n# convert to RDD\npeople_rdd = sc.parallelize(people_json)\n# convert to dataframe\npeople_df = spark.read.json(people_rdd)\n# rearrange column order\npeople_df = people_df.select(\"age\", \"name\", \"cars\", \"locations\")\n\ndisplay(people_df)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>age</th><th>name</th><th>cars</th><th>locations</th></tr></thead><tbody><tr><td>34</td><td>List(Alice, Summers)</td><td>List(honda)</td><td>List(List(Glasgow, work), List(Edinburgh, home))</td></tr><tr><td>33</td><td>List(Bob, Winters)</td><td>List(ford, BMW)</td><td>List(List(Glasgow, work), List(Edinburgh, home))</td></tr><tr><td>35</td><td>List(Charlie, Spring)</td><td>List(vauxhall, peugeot, VW)</td><td>List(List(Bristol, work), List(Bath, home), List(Paris, holiday))</td></tr></tbody></table></div>"]}}],"execution_count":2},{"cell_type":"code","source":["people_df.printSchema()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">root\n-- age: string (nullable = true)\n-- name: struct (nullable = true)\n    |-- first: string (nullable = true)\n    |-- last: string (nullable = true)\n-- cars: array (nullable = true)\n    |-- element: string (containsNull = true)\n-- locations: array (nullable = true)\n    |-- element: struct (containsNull = true)\n    |    |-- city: string (nullable = true)\n    |    |-- type: string (nullable = true)\n\n</div>"]}}],"execution_count":3},{"cell_type":"code","source":["# extracting data from a map (dictionary) column\nsurname_df = people_df.withColumn(\"surname\", fn.col(\"name\")[\"last\"])\ndisplay(surname_df)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>age</th><th>name</th><th>cars</th><th>locations</th><th>surname</th></tr></thead><tbody><tr><td>34</td><td>List(Alice, Summers)</td><td>List(honda)</td><td>List(List(Glasgow, work), List(Edinburgh, home))</td><td>Summers</td></tr><tr><td>33</td><td>List(Bob, Winters)</td><td>List(ford, BMW)</td><td>List(List(Glasgow, work), List(Edinburgh, home))</td><td>Winters</td></tr><tr><td>35</td><td>List(Charlie, Spring)</td><td>List(vauxhall, peugeot, VW)</td><td>List(List(Bristol, work), List(Bath, home), List(Paris, holiday))</td><td>Spring</td></tr></tbody></table></div>"]}}],"execution_count":4},{"cell_type":"code","source":["# extracting data from an array (list) columnn\nfirst_car = people_df.withColumn(\"first_car\", fn.col(\"cars\")[0])\ndisplay(first_car)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>age</th><th>name</th><th>cars</th><th>locations</th><th>first_car</th></tr></thead><tbody><tr><td>34</td><td>List(Alice, Summers)</td><td>List(honda)</td><td>List(List(Glasgow, work), List(Edinburgh, home))</td><td>honda</td></tr><tr><td>33</td><td>List(Bob, Winters)</td><td>List(ford, BMW)</td><td>List(List(Glasgow, work), List(Edinburgh, home))</td><td>ford</td></tr><tr><td>35</td><td>List(Charlie, Spring)</td><td>List(vauxhall, peugeot, VW)</td><td>List(List(Bristol, work), List(Bath, home), List(Paris, holiday))</td><td>vauxhall</td></tr></tbody></table></div>"]}}],"execution_count":5},{"cell_type":"code","source":["# extracting a defined value to a new dataframe\nsingle_value_people_df = people_df.withColumn(\"first_city\", fn.col(\"locations\")[0][\"city\"])\ndisplay(single_value_people_df)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>age</th><th>name</th><th>cars</th><th>locations</th><th>first_city</th></tr></thead><tbody><tr><td>34</td><td>List(Alice, Summers)</td><td>List(honda)</td><td>List(List(Glasgow, work), List(Edinburgh, home))</td><td>Glasgow</td></tr><tr><td>33</td><td>List(Bob, Winters)</td><td>List(ford, BMW)</td><td>List(List(Glasgow, work), List(Edinburgh, home))</td><td>Glasgow</td></tr><tr><td>35</td><td>List(Charlie, Spring)</td><td>List(vauxhall, peugeot, VW)</td><td>List(List(Bristol, work), List(Bath, home), List(Paris, holiday))</td><td>Bristol</td></tr></tbody></table></div>"]}}],"execution_count":6},{"cell_type":"code","source":["expanded_people_df = people_df.withColumn(\"cities\", fn.col(\"locations\")[\"city\"])"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":7},{"cell_type":"code","source":["display(expanded_people_df)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>age</th><th>name</th><th>cars</th><th>locations</th><th>cities</th></tr></thead><tbody><tr><td>34</td><td>List(Alice, Summers)</td><td>List(honda)</td><td>List(List(Glasgow, work), List(Edinburgh, home))</td><td>List(Glasgow, Edinburgh)</td></tr><tr><td>33</td><td>List(Bob, Winters)</td><td>List(ford, BMW)</td><td>List(List(Glasgow, work), List(Edinburgh, home))</td><td>List(Glasgow, Edinburgh)</td></tr><tr><td>35</td><td>List(Charlie, Spring)</td><td>List(vauxhall, peugeot, VW)</td><td>List(List(Bristol, work), List(Bath, home), List(Paris, holiday))</td><td>List(Bristol, Bath, Paris)</td></tr></tbody></table></div>"]}}],"execution_count":8},{"cell_type":"code","source":["extracted_people_df = people_df.withColumn(\"location\", fn.col(\"locations\")[0])"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":9},{"cell_type":"code","source":["display(extracted_people_df)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>age</th><th>name</th><th>cars</th><th>locations</th><th>location</th></tr></thead><tbody><tr><td>34</td><td>List(Alice, Summers)</td><td>List(honda)</td><td>List(List(Glasgow, work), List(Edinburgh, home))</td><td>List(Glasgow, work)</td></tr><tr><td>33</td><td>List(Bob, Winters)</td><td>List(ford, BMW)</td><td>List(List(Glasgow, work), List(Edinburgh, home))</td><td>List(Glasgow, work)</td></tr><tr><td>35</td><td>List(Charlie, Spring)</td><td>List(vauxhall, peugeot, VW)</td><td>List(List(Bristol, work), List(Bath, home), List(Paris, holiday))</td><td>List(Bristol, work)</td></tr></tbody></table></div>"]}}],"execution_count":10},{"cell_type":"code","source":["# Alternatively, a DataFrame can be created for a JSON dataset represented by\n# an RDD[String] storing one JSON object per string\nclass_json = ['[{\"subject\":\"maths\", \"performance\":{\"names\": [\"Alice\", \"Bob\", \"Charlie\", \"Daniel\", \"Emily\"], \"scores\":[45,56,67,34,89], \"grades\": [\"D\", \"C\", \"B\", \"E\", \"A\"]}},\\\n                 {\"subject\":\"english\", \"performance\":{\"names\": [\"Alice\", \"Bob\", \"Charlie\", \"Daniel\", \"Emily\"], \"scores\":[79,54,62,39,64], \"grades\": [\"A\", \"C\", \"B\", \"E\", \"B\"]}},\\\n                 {\"subject\":\"history\", \"performance\":{\"names\": [\"Alice\", \"Bob\", \"Charlie\", \"Daniel\", \"Emily\"], \"scores\":[47,76,61,44,79], \"grades\": [\"D\", \"A\", \"B\", \"D\", \"A\"]}}]']\nclass_rdd = sc.parallelize(class_json)\nclass_df = spark.read.json(class_rdd)\nclass_df = class_df.select(\"subject\", \"performance\")\nclass_df.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-------+--------------------+\nsubject|         performance|\n+-------+--------------------+\n  maths|[[D, C, B, E, A],...|\nenglish|[[A, C, B, E, B],...|\nhistory|[[D, A, B, D, A],...|\n+-------+--------------------+\n\n</div>"]}}],"execution_count":11},{"cell_type":"code","source":["display(class_df)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>subject</th><th>performance</th></tr></thead><tbody><tr><td>maths</td><td>List(List(D, C, B, E, A), List(Alice, Bob, Charlie, Daniel, Emily), List(45, 56, 67, 34, 89))</td></tr><tr><td>english</td><td>List(List(A, C, B, E, B), List(Alice, Bob, Charlie, Daniel, Emily), List(79, 54, 62, 39, 64))</td></tr><tr><td>history</td><td>List(List(D, A, B, D, A), List(Alice, Bob, Charlie, Daniel, Emily), List(47, 76, 61, 44, 79))</td></tr></tbody></table></div>"]}}],"execution_count":12},{"cell_type":"code","source":["# we can pull out a specified value from a map of arrays to a new column\ngrade_df = class_df.withColumn(\"first_grade\", fn.col(\"performance\")[\"grades\"][0])"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":13},{"cell_type":"code","source":["display(grade_df)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>subject</th><th>performance</th><th>first_student_grade</th></tr></thead><tbody><tr><td>maths</td><td>List(List(D, C, B, E, A), List(Alice, Bob, Charlie, Daniel, Emily), List(45, 56, 67, 34, 89))</td><td>D</td></tr><tr><td>english</td><td>List(List(A, C, B, E, B), List(Alice, Bob, Charlie, Daniel, Emily), List(79, 54, 62, 39, 64))</td><td>A</td></tr><tr><td>history</td><td>List(List(D, A, B, D, A), List(Alice, Bob, Charlie, Daniel, Emily), List(47, 76, 61, 44, 79))</td><td>D</td></tr></tbody></table></div>"]}}],"execution_count":14},{"cell_type":"code","source":["# however we cannot directly extract all first elements from a map of arrays\nextracted_class_df = class_df.withColumn(\"first_student\", fn.col(\"performance\")[0])\n# this gives an ERROR!"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">Py4JJavaError</span>                             Traceback (most recent call last)\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansi-cyan-fg\">deco</span><span class=\"ansi-blue-fg\">(*a, **kw)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     62</span>         <span class=\"ansi-green-fg\">try</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">---&gt; 63</span><span class=\"ansi-red-fg\">             </span><span class=\"ansi-green-fg\">return</span> f<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>a<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kw<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     64</span>         <span class=\"ansi-green-fg\">except</span> py4j<span class=\"ansi-blue-fg\">.</span>protocol<span class=\"ansi-blue-fg\">.</span>Py4JJavaError <span class=\"ansi-green-fg\">as</span> e<span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py</span> in <span class=\"ansi-cyan-fg\">get_return_value</span><span class=\"ansi-blue-fg\">(answer, gateway_client, target_id, name)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    327</span>                     <span class=\"ansi-blue-fg\">&#34;An error occurred while calling {0}{1}{2}.\\n&#34;</span><span class=\"ansi-blue-fg\">.</span>\n<span class=\"ansi-green-fg\">--&gt; 328</span><span class=\"ansi-red-fg\">                     format(target_id, &#34;.&#34;, name), value)\n</span><span class=\"ansi-green-intense-fg ansi-bold\">    329</span>             <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-red-fg\">Py4JJavaError</span>: An error occurred while calling o914.withColumn.\n: org.apache.spark.sql.AnalysisException: Field name should be String Literal, but it&#39;s 0;\n\tat org.apache.spark.sql.catalyst.expressions.ExtractValue$.apply(complexTypeExtractors.scala:73)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveReferences$$resolve(Analyzer.scala:914)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveReferences$$resolve$2.apply(Analyzer.scala:915)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveReferences$$resolve$2.apply(Analyzer.scala:915)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$8.apply(TreeNode.scala:353)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:207)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:351)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveReferences$$resolve(Analyzer.scala:915)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$apply$9$$anonfun$applyOrElse$36.apply(Analyzer.scala:993)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$apply$9$$anonfun$applyOrElse$36.apply(Analyzer.scala:993)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$3.apply(QueryPlan.scala:106)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$3.apply(QueryPlan.scala:106)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:76)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:105)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:117)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1$2.apply(QueryPlan.scala:122)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:104)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:122)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$4.apply(QueryPlan.scala:127)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:207)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:127)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$apply$9.applyOrElse(Analyzer.scala:993)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$apply$9.applyOrElse(Analyzer.scala:918)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$apply$1.apply(AnalysisHelper.scala:90)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$apply$1.apply(AnalysisHelper.scala:90)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:76)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:89)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:194)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.resolveOperatorsUp(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$.apply(Analyzer.scala:918)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$.apply(Analyzer.scala:775)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:112)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:109)\n\tat scala.collection.LinearSeqOptimized$class.foldLeft(LinearSeqOptimized.scala:124)\n\tat scala.collection.immutable.List.foldLeft(List.scala:84)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:109)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:101)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:101)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:137)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:131)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:103)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$executeAndTrack$1.apply(RuleExecutor.scala:80)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$executeAndTrack$1.apply(RuleExecutor.scala:80)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:88)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:79)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:115)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:114)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:114)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$analyzed$1.apply(QueryExecution.scala:85)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$analyzed$1.apply(QueryExecution.scala:82)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:82)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:82)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:74)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:80)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withPlan(Dataset.scala:3527)\n\tat org.apache.spark.sql.Dataset.select(Dataset.scala:1361)\n\tat org.apache.spark.sql.Dataset.withColumns(Dataset.scala:2326)\n\tat org.apache.spark.sql.Dataset.withColumn(Dataset.scala:2293)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\n\n\nDuring handling of the above exception, another exception occurred:\n\n<span class=\"ansi-red-fg\">AnalysisException</span>                         Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-668063455150104&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-fg\">----&gt; 1</span><span class=\"ansi-red-fg\"> </span>extracted_class_df <span class=\"ansi-blue-fg\">=</span> class_df<span class=\"ansi-blue-fg\">.</span>withColumn<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;first_student&#34;</span><span class=\"ansi-blue-fg\">,</span> fn<span class=\"ansi-blue-fg\">.</span>col<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;performance&#34;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">0</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/dataframe.py</span> in <span class=\"ansi-cyan-fg\">withColumn</span><span class=\"ansi-blue-fg\">(self, colName, col)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   2020</span>         &#34;&#34;&#34;\n<span class=\"ansi-green-intense-fg ansi-bold\">   2021</span>         <span class=\"ansi-green-fg\">assert</span> isinstance<span class=\"ansi-blue-fg\">(</span>col<span class=\"ansi-blue-fg\">,</span> Column<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">&#34;col should be Column&#34;</span>\n<span class=\"ansi-green-fg\">-&gt; 2022</span><span class=\"ansi-red-fg\">         </span><span class=\"ansi-green-fg\">return</span> DataFrame<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">.</span>_jdf<span class=\"ansi-blue-fg\">.</span>withColumn<span class=\"ansi-blue-fg\">(</span>colName<span class=\"ansi-blue-fg\">,</span> col<span class=\"ansi-blue-fg\">.</span>_jc<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">,</span> self<span class=\"ansi-blue-fg\">.</span>sql_ctx<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   2023</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">   2024</span>     <span class=\"ansi-blue-fg\">@</span>ignore_unicode_prefix\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py</span> in <span class=\"ansi-cyan-fg\">__call__</span><span class=\"ansi-blue-fg\">(self, *args)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1255</span>         answer <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>gateway_client<span class=\"ansi-blue-fg\">.</span>send_command<span class=\"ansi-blue-fg\">(</span>command<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1256</span>         return_value = get_return_value(\n<span class=\"ansi-green-fg\">-&gt; 1257</span><span class=\"ansi-red-fg\">             answer, self.gateway_client, self.target_id, self.name)\n</span><span class=\"ansi-green-intense-fg ansi-bold\">   1258</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">   1259</span>         <span class=\"ansi-green-fg\">for</span> temp_arg <span class=\"ansi-green-fg\">in</span> temp_args<span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansi-cyan-fg\">deco</span><span class=\"ansi-blue-fg\">(*a, **kw)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     67</span>                                              e.java_exception.getStackTrace()))\n<span class=\"ansi-green-intense-fg ansi-bold\">     68</span>             <span class=\"ansi-green-fg\">if</span> s<span class=\"ansi-blue-fg\">.</span>startswith<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;org.apache.spark.sql.AnalysisException: &#39;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">---&gt; 69</span><span class=\"ansi-red-fg\">                 </span><span class=\"ansi-green-fg\">raise</span> AnalysisException<span class=\"ansi-blue-fg\">(</span>s<span class=\"ansi-blue-fg\">.</span>split<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;: &#39;</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> stackTrace<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     70</span>             <span class=\"ansi-green-fg\">if</span> s<span class=\"ansi-blue-fg\">.</span>startswith<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;org.apache.spark.sql.catalyst.analysis&#39;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     71</span>                 <span class=\"ansi-green-fg\">raise</span> AnalysisException<span class=\"ansi-blue-fg\">(</span>s<span class=\"ansi-blue-fg\">.</span>split<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;: &#39;</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> stackTrace<span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-red-fg\">AnalysisException</span>: &#34;Field name should be String Literal, but it&#39;s 0;&#34;</div>"]}}],"execution_count":15},{"cell_type":"code","source":["# using .* to extract a struct column to separate columns for onward processing\nseparated_class_df = class_df.select(\"subject\", \"performance.*\")\n    \ndisplay(separated_class_df)    "],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>subject</th><th>grades</th><th>names</th><th>scores</th></tr></thead><tbody><tr><td>maths</td><td>List(D, C, B, E, A)</td><td>List(Alice, Bob, Charlie, Daniel, Emily)</td><td>List(45, 56, 67, 34, 89)</td></tr><tr><td>english</td><td>List(A, C, B, E, B)</td><td>List(Alice, Bob, Charlie, Daniel, Emily)</td><td>List(79, 54, 62, 39, 64)</td></tr><tr><td>history</td><td>List(D, A, B, D, A)</td><td>List(Alice, Bob, Charlie, Daniel, Emily)</td><td>List(47, 76, 61, 44, 79)</td></tr></tbody></table></div>"]}}],"execution_count":16},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":17}],"metadata":{"name":"PySpark Array and Map columns","notebookId":3877597026823542},"nbformat":4,"nbformat_minor":0}
