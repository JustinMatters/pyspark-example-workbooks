{"cells":[{"cell_type":"code","source":["import pandas as pd\nfrom pyspark.sql import functions as fn"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":1},{"cell_type":"code","source":["# lets define a simple dataframe\n\ndata = {'A': ['a', 'b', None], \n        'B': [0, 1, 0],\n        'C': [1, 0, 1],\n        'D': [1, 0, 0]}\ndf = pd.DataFrame(data)\ndf = spark.createDataFrame(df)\ndisplay(df)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>A</th><th>B</th><th>C</th><th>D</th></tr></thead><tbody><tr><td>a</td><td>0</td><td>1</td><td>1</td></tr><tr><td>b</td><td>1</td><td>0</td><td>0</td></tr><tr><td>null</td><td>0</td><td>1</td><td>0</td></tr></tbody></table></div>"]}}],"execution_count":2},{"cell_type":"code","source":["# now lets filter with a condition and its inverse - watch what happens to the row with a null value\ndf.show()\ndf.filter(df.A == \"a\").show()\ndf.filter(df.A != \"a\").show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----+---+---+---+\n   A|  B|  C|  D|\n+----+---+---+---+\n   a|  0|  1|  1|\n   b|  1|  0|  0|\nnull|  0|  1|  0|\n+----+---+---+---+\n\n+---+---+---+---+\n  A|  B|  C|  D|\n+---+---+---+---+\n  a|  0|  1|  1|\n+---+---+---+---+\n\n+---+---+---+---+\n  A|  B|  C|  D|\n+---+---+---+---+\n  b|  1|  0|  0|\n+---+---+---+---+\n\n</div>"]}}],"execution_count":3},{"cell_type":"code","source":["# because of pysparks SQL compatibility column names are not case sensitive\ndf2 = df.withColumnRenamed(\"B\", \"a\")\ndf2.show()\n# df2.select(\"A\").show()\ndf2.select(\"a\").show()\ndf2.withColumnRenamed(\"A\", \"e\").show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">---------------------------------------------------------------------------</span>\n<span class=\"ansired\">AnalysisException</span>                         Traceback (most recent call last)\n<span class=\"ansigreen\">&lt;command-929035903441204&gt;</span> in <span class=\"ansicyan\">&lt;module&gt;</span><span class=\"ansiblue\">()</span>\n<span class=\"ansigreen\">      2</span> df2<span class=\"ansiyellow\">.</span>show<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">      3</span> <span class=\"ansired\"># df2.select(&quot;A&quot;).show()</span><span class=\"ansiyellow\"></span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">----&gt; 4</span><span class=\"ansiyellow\"> </span>df2<span class=\"ansiyellow\">.</span>select<span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&quot;a&quot;</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">.</span>show<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">      5</span> df2<span class=\"ansiyellow\">.</span>withColumnRenamed<span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&quot;A&quot;</span><span class=\"ansiyellow\">,</span> <span class=\"ansiblue\">&quot;e&quot;</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">.</span>show<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/sql/dataframe.py</span> in <span class=\"ansicyan\">select</span><span class=\"ansiblue\">(self, *cols)</span>\n<span class=\"ansigreen\">   1343</span>         <span class=\"ansiyellow\">[</span>Row<span class=\"ansiyellow\">(</span>name<span class=\"ansiyellow\">=</span><span class=\"ansiblue\">u&apos;Alice&apos;</span><span class=\"ansiyellow\">,</span> age<span class=\"ansiyellow\">=</span><span class=\"ansicyan\">12</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">,</span> Row<span class=\"ansiyellow\">(</span>name<span class=\"ansiyellow\">=</span><span class=\"ansiblue\">u&apos;Bob&apos;</span><span class=\"ansiyellow\">,</span> age<span class=\"ansiyellow\">=</span><span class=\"ansicyan\">15</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">]</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">   1344</span>         &quot;&quot;&quot;\n<span class=\"ansigreen\">-&gt; 1345</span><span class=\"ansiyellow\">         </span>jdf <span class=\"ansiyellow\">=</span> self<span class=\"ansiyellow\">.</span>_jdf<span class=\"ansiyellow\">.</span>select<span class=\"ansiyellow\">(</span>self<span class=\"ansiyellow\">.</span>_jcols<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">*</span>cols<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">   1346</span>         <span class=\"ansigreen\">return</span> DataFrame<span class=\"ansiyellow\">(</span>jdf<span class=\"ansiyellow\">,</span> self<span class=\"ansiyellow\">.</span>sql_ctx<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">   1347</span> <span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py</span> in <span class=\"ansicyan\">__call__</span><span class=\"ansiblue\">(self, *args)</span>\n<span class=\"ansigreen\">   1255</span>         answer <span class=\"ansiyellow\">=</span> self<span class=\"ansiyellow\">.</span>gateway_client<span class=\"ansiyellow\">.</span>send_command<span class=\"ansiyellow\">(</span>command<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">   1256</span>         return_value = get_return_value(\n<span class=\"ansigreen\">-&gt; 1257</span><span class=\"ansiyellow\">             answer, self.gateway_client, self.target_id, self.name)\n</span><span class=\"ansigreen\">   1258</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">   1259</span>         <span class=\"ansigreen\">for</span> temp_arg <span class=\"ansigreen\">in</span> temp_args<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansicyan\">deco</span><span class=\"ansiblue\">(*a, **kw)</span>\n<span class=\"ansigreen\">     67</span>                                              e.java_exception.getStackTrace()))\n<span class=\"ansigreen\">     68</span>             <span class=\"ansigreen\">if</span> s<span class=\"ansiyellow\">.</span>startswith<span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&apos;org.apache.spark.sql.AnalysisException: &apos;</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">---&gt; 69</span><span class=\"ansiyellow\">                 </span><span class=\"ansigreen\">raise</span> AnalysisException<span class=\"ansiyellow\">(</span>s<span class=\"ansiyellow\">.</span>split<span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&apos;: &apos;</span><span class=\"ansiyellow\">,</span> <span class=\"ansicyan\">1</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">[</span><span class=\"ansicyan\">1</span><span class=\"ansiyellow\">]</span><span class=\"ansiyellow\">,</span> stackTrace<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     70</span>             <span class=\"ansigreen\">if</span> s<span class=\"ansiyellow\">.</span>startswith<span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&apos;org.apache.spark.sql.catalyst.analysis&apos;</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     71</span>                 <span class=\"ansigreen\">raise</span> AnalysisException<span class=\"ansiyellow\">(</span>s<span class=\"ansiyellow\">.</span>split<span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&apos;: &apos;</span><span class=\"ansiyellow\">,</span> <span class=\"ansicyan\">1</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">[</span><span class=\"ansicyan\">1</span><span class=\"ansiyellow\">]</span><span class=\"ansiyellow\">,</span> stackTrace<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansired\">AnalysisException</span>: &quot;Reference &apos;a&apos; is ambiguous, could be: a, a.;&quot;</div>"]}}],"execution_count":4},{"cell_type":"code","source":[""],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----+\n   a|\n+----+\n   a|\n   b|\nnull|\n+----+\n\n</div>"]}}],"execution_count":5},{"cell_type":"code","source":["# sufficiently large ints will roll over to null without raising an error\n# also floats seem to be able to hold larger abosolute values than they can operate on\n\ndata = {'LongIntCol': [1234567890, 123456789012345678, 1, 12345678901234567890, 1],\n        'FloatCol': [1.0, 1.0, 123456789012345678, 1.0, 12345678901234567890.0]\n       }\ndf = pd.DataFrame(data)\ndf = spark.createDataFrame(df)\ndf = df.withColumn(\"Multiplied\", df.LongIntCol*df.FloatCol)\ndisplay(df)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>FloatCol</th><th>LongIntCol</th><th>Multiplied</th></tr></thead><tbody><tr><td>1.0</td><td>1234567890</td><td>1.23456789E9</td></tr><tr><td>1.0</td><td>123456789012345678</td><td>1.2345678901234568E17</td></tr><tr><td>1.2345678901234568E17</td><td>1</td><td>1.2345678901234568E17</td></tr><tr><td>1.0</td><td>null</td><td>null</td></tr><tr><td>1.2345678901234567E19</td><td>1</td><td>1.2345678901234567E19</td></tr></tbody></table></div>"]}}],"execution_count":6},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":7}],"metadata":{"name":"PySparkSurprises","notebookId":929035903441196},"nbformat":4,"nbformat_minor":0}
