{"cells":[{"cell_type":"code","source":["from pyspark.sql import functions as fn, types as T\nfrom pyspark.sql import Window\nimport pandas as pd\n# import panda_udf difect so we can use it as a decorator\nfrom pyspark.sql.functions import pandas_udf, PandasUDFType"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"ff627312-1704-4c84-995e-7bf9d4edfcef","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{},"output_type":"display_data","data":{"text/plain":[],"application/vnd.databricks.v1+bamboolib_hint":"{\"pd.DataFrames\": [], \"version\": \"0.0.1\"}"}}],"execution_count":0},{"cell_type":"code","source":["df_array = spark.createDataFrame(\n    [\n      (1, [1,2,3]), \n      (2, [4,5,6]), \n      (3, [7,8,9]), \n      (1, [2,2,2]), \n      (2, [5,5,5]), \n      (3, [8,8,8])\n    ], \n  (\"group\", \"array\")\n)\n\n# this display will only sum the group column, not the array column\ndisplay(df_array.groupBy().sum())"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"fb5facdd-4409-470a-9c45-3e97b855602e","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[[12]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"sum(group)","type":"\"long\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n","  .table-result-container {\n","    max-height: 300px;\n","    overflow: auto;\n","  }\n","  table, th, td {\n","    border: 1px solid black;\n","    border-collapse: collapse;\n","  }\n","  th, td {\n","    padding: 5px;\n","  }\n","  th {\n","    text-align: left;\n","  }\n","</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>sum(group)</th></tr></thead><tbody><tr><td>12</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"code","source":["@pandas_udf(T.ArrayType(T.IntegerType()))\ndef sum_array(input: pd.Series) -> float:\n  # now we can use pandas sum which does handle arrays\n  return input.sum() \n\n@pandas_udf(T.ArrayType(T.FloatType()))\ndef avg_array(input: pd.Series) -> float:\n  # now we can use pandas mean which does handle arrays\n  return input.mean() \n\nwindow = Window.partitionBy(\n  'group'\n).rowsBetween(\n  Window.unboundedPreceding,\n  Window.unboundedFollowing\n)\n\ndf_out = df_array.withColumn(\n  'sum_array', sum_array('array').over(window)\n).withColumn(\n  'avg_array', avg_array('array').over(window)\n)\n\ndisplay(df_out)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"48912746-06d0-40fd-8097-689ef53db8dd","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[[1,[1,2,3],[3,4,5],[1.5,2.0,2.5]],[1,[2,2,2],[3,4,5],[1.5,2.0,2.5]],[2,[4,5,6],[9,10,11],[4.5,5.0,5.5]],[2,[5,5,5],[9,10,11],[4.5,5.0,5.5]],[3,[7,8,9],[15,16,17],[7.5,8.0,8.5]],[3,[8,8,8],[15,16,17],[7.5,8.0,8.5]]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"group","type":"\"long\"","metadata":"{}"},{"name":"array","type":"{\"type\":\"array\",\"elementType\":\"long\",\"containsNull\":true}","metadata":"{}"},{"name":"sum_array","type":"{\"type\":\"array\",\"elementType\":\"integer\",\"containsNull\":true}","metadata":"{}"},{"name":"avg_array","type":"{\"type\":\"array\",\"elementType\":\"float\",\"containsNull\":true}","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n","  .table-result-container {\n","    max-height: 300px;\n","    overflow: auto;\n","  }\n","  table, th, td {\n","    border: 1px solid black;\n","    border-collapse: collapse;\n","  }\n","  th, td {\n","    padding: 5px;\n","  }\n","  th {\n","    text-align: left;\n","  }\n","</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>group</th><th>array</th><th>sum_array</th><th>avg_array</th></tr></thead><tbody><tr><td>1</td><td>List(1, 2, 3)</td><td>List(3, 4, 5)</td><td>List(1.5, 2.0, 2.5)</td></tr><tr><td>1</td><td>List(2, 2, 2)</td><td>List(3, 4, 5)</td><td>List(1.5, 2.0, 2.5)</td></tr><tr><td>2</td><td>List(4, 5, 6)</td><td>List(9, 10, 11)</td><td>List(4.5, 5.0, 5.5)</td></tr><tr><td>2</td><td>List(5, 5, 5)</td><td>List(9, 10, 11)</td><td>List(4.5, 5.0, 5.5)</td></tr><tr><td>3</td><td>List(7, 8, 9)</td><td>List(15, 16, 17)</td><td>List(7.5, 8.0, 8.5)</td></tr><tr><td>3</td><td>List(8, 8, 8)</td><td>List(15, 16, 17)</td><td>List(7.5, 8.0, 8.5)</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# in older versions of PySpark (eg 2.4) you can try this\n\n# @pandas_udf(T.ArrayType(T.IntegerType()), PandasUDFType.GROUPED_AGG)\n# def sum_array(input):\n#   return input.sum() \n\n# @pandas_udf(T.ArrayType(T.FloatType()), PandasUDFType.GROUPED_AGG)\n# def avg_array(input):\n#   return input.mean() \n\n# window = (Window\n#     .partitionBy('group')\n#     .rowsBetween(Window.unboundedPreceding, Window.unboundedFollowing))\n\n# df_out = df_array.withColumn(\n#   'sum_array', sum_array('array').over(window)\n# ).withColumn(\n#   'avg_array', avg_array('array').over(window)\n# )\n#\n# display(df_out)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"5208581a-437e-4c61-99c4-4768d583671f","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"PySpark Elementwise Array Aggregations","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{}}},"nbformat":4,"nbformat_minor":0}
